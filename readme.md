# ğŸ•·ï¸ Go Web Crawler

A simple yet powerful **concurrent web crawler** written in Go. This tool crawls websites starting from a given URL, traverses links up to a specified depth, extracts page titles, and saves the results to a CSV file.

---

## ğŸ“Œ Features

- ğŸ”— Crawl any website from a start URL
- âš¡ Concurrent crawling with goroutines
- ğŸ¯ Depth-based traversal to control recursion level
- ğŸ” Prevents duplicate visits with a thread-safe visited map
- ğŸ§  Extracts `<title>` tags from HTML pages
- ğŸ“„ Saves crawl results (URL + Title) to `output.csv`
- ğŸ“¦ Modular architecture using idiomatic Go packages
- ğŸ› ï¸ CLI-based usage for flexibility

### âœ… Prerequisites

- Go 1.19 or higher
- Internet connection

### ğŸ“¦ Install Dependencies

```bash
go mod tidy
```

### Run the crawler

```bash
go run main.go https://example.com 2
```

## ğŸ“Œ To-Do (Improvements)

- [ ] Respect `robots.txt`
- [ ] Add rate limiting to avoid server overload
- [ ] Add SQLite or JSON output formats
- [ ] Add domain filter or link validator
- [ ] Add unit and integration tests

---

## ğŸ§‘â€ğŸ’» Author

**Amey Mahajan**  
[LinkedIn](https://www.linkedin.com/in/mahajanamu/)
